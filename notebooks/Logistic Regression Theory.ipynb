{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to go through theory behind logistic regression with formulas and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats._continuous_distns import _distn_names\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a great tool for examining relationships between a set of 'independent variables' (X) and a dependent variable (y)\n",
    "\n",
    "e.g. how correlated is someone's height (y) with their {weight, age, sex, ethnicity}?\n",
    "\n",
    "This is expressed mathematically as:\n",
    "\n",
    "$$y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n + \\epsilon$$\n",
    "\n",
    "where we attach weights / sensitivities $\\beta$ to each of our Xs with $\\alpha$ an overall 'bias'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem then becomes finding the above weights to best fit our observed data - we can define this as minimising the square between $\\hat{y}$ and $y$ where $\\hat{y}$ is the prediction that our above model would spit out\n",
    "\n",
    "$$min_{\\alpha, \\beta} \\sum_i^n{(y - \\hat{y})^2}$$\n",
    "\n",
    "Substituting in:\n",
    "$$min_{\\alpha, \\beta} \\sum_i^n{(y - (\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n))^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then differentiate this wrt our n+1 params to get n+1 eq and n+1 unknowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Issue for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model assumes that y is a continuous unbounded variable - this doesn't work if we are looking at categorical or binary variables\n",
    "\n",
    "Instead we need something to 'squash' this onto the interval (0,1) where we can then interpret the value as the probability of getting y=1 (for binary classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issues with using linear regression for binary classification are well explained [here](https://christophm.github.io/interpretable-ml-book/logistic.html#what-is-wrong-with-linear-regression-for-classification):\n",
    "\n",
    "\"Since the predicted outcome is not a probability, but a linear interpolation between points, there is no meaningful threshold at which you can distinguish one class from the other. A good illustration of this issue has been given on Stackoverflow.\n",
    "\n",
    "Linear models do not extend to classification problems with multiple classes. You would have to start labeling the next class with 2, then 3, and so on. The classes might not have any meaningful order, but the linear model would force a weird structure on the relationship between the features and your class predictions. The higher the value of a feature with a positive weight, the more it contributes to the prediction of a class with a higher number, even if classes that happen to get a similar number are not closer than other classes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic / Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function takes an input x and maps it to the interval (0,1) - this is what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = list(range(-10,10))\n",
    "\n",
    "def sigmoid(x):\n",
    "    l = 1 / (1 + math.exp(-x))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5));\n",
    "sns.lineplot(data=pd.DataFrame(data={'y':[sigmoid(x) for x in x_vals], 'x': x_vals}), y='y', x='x', ax=ax);\n",
    "\n",
    "ax.set_ylabel('sigmoid(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression Function & Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then combine the two functions above to get the form of our logistic regression:\n",
    "\n",
    "$$y = P(y=1) =  \\frac{1}{1 + e^{-[\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n]}}$$\n",
    "\n",
    "This then means that $P(y=0)$ is expressed as:\n",
    "\n",
    "$$P(y=0) = 1 - P(y=1) = \\frac{e^{-[\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n]}}{1 + e^{-[\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can ratio these 2 quantities to try to understand what the parameters we estimate mean intuitively:\n",
    "\n",
    "$$\\frac{P(y=1)}{P(y=0)} = e^{[\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n]}$$\n",
    "\n",
    "And then taking logs of both sides:\n",
    "\n",
    "$$log(\\frac{P(y=1)}{P(y=0)}) = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we are estimating a linear model for the log odds of the event y=1, where odds are defined [as](https://en.wikipedia.org/wiki/Odds#Mathematical_relations):\n",
    "\n",
    "$$odds = \\frac{P(y=1)}{1 - P(y=1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see what the parameters mean if we e.g. increase $x_1$ by a small increment\n",
    "\n",
    "Taking the ratio of the odds with a small increase in x vs before we get:\n",
    "\n",
    "$$\\frac{odds_{x_{1+1}}}{odds} = \\frac{e^{[\\alpha + \\beta_1 (x_1 + 1) + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n]}}{e^{[\\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 +...+ \\beta_n x_n]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the rules of logs we know we can cancel out most of the other terms to get:\n",
    "\n",
    "$$\\frac{odds_{x_{1+1}}}{odds} = e^{\\beta_1 (x_1 + 1) - \\beta_1 (x_1)} = e^{\\beta_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[As stated here](https://christophm.github.io/interpretable-ml-book/logistic.html#interpretation-1):\n",
    " - Each parameter affects the change in the odds in a non-linear way due to the exponential function\n",
    " - As for interpretation of $\\beta$:\n",
    "  - \"For example, if you have odds of 2, it means that the probability for y=1 is twice as high as y=0. If you have a weight (= log odds ratio) of 0.7, then increasing the respective feature by one unit multiplies the odds by exp(0.7) (approximately 2) and the odds change to 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ML Approach to Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML literature treats logistic regression a bit differently from classical stats / econometrics, in a form that is more generisable to multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the problem, let's say:\n",
    " - We have K classes that we want to classify our data observations into\n",
    " - For each observation, we have the following pair {y, __x__}, where:\n",
    "  - y is the 'target variable' of what class the observation belongs to\n",
    "  - __x__ is a vector of observable data points that will help us fit our classifier i.e. a set of 'features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem becomes: How do we train a classifier s.t. we end up with a model/function that we can feed in new observations (sets of __x__) and generate a class prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think about it as wanting the following output:\n",
    " - A k size vector of outputs that gives us the probability that the obs belongs to that class\n",
    " - The sum of the vector is 1 so that we have a 'probability distribution'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this, we can think of modelling each class as a separate log-linear regression, like the following:\n",
    "\n",
    "For $N$ observations from $i=1,...,N$ and for $K$ classes from $k=1,...,K$, and for a matrix $\\pmb{X}$ NxM dims of N obs and M features:\n",
    "\n",
    "$$ln P(y_i = k) = \\pmb{\\beta_k} \\pmb{X_i} - ln Z$$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$$P(y_i = k) = \\frac{e^{\\pmb{\\beta_k} \\pmb{X_i}}}{Z}$$\n",
    "\n",
    "where we choose $Z$ as a normalising factor to ensure:\n",
    "\n",
    "$$\\sum_{i=1}^{K} P(y_i=k) = 1$$\n",
    "\n",
    "for every observation $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what Z should be by re-arranging the above:\n",
    "\n",
    "$$\\sum_{i=1}^{K} \\frac{e^{\\pmb{\\beta_k} \\pmb{X_i}}}{Z} = 1$$\n",
    "\n",
    "giving us:\n",
    "\n",
    "$$Z = \\sum_{i=1}^{K}e^{\\pmb{\\beta_k} \\pmb{X_i}}$$\n",
    "\n",
    "i.e. Z is a constant across all classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This then gives us the following formulation for the probability that an observation $y_i$ belongs to a class $c$:\n",
    "\n",
    "$$P(y_i = c) = \\frac{e^{\\pmb{\\beta_c} \\pmb{X_i}}}{\\sum_{i=1}^{K}e^{\\pmb{\\beta_k} \\pmb{X_i}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formulation is known as a [softmax function](https://en.wikipedia.org/wiki/Softmax_function) and bounds our estimates onto the interval $P(x) \\in{(0,1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Estimating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the above, for $K$ classes and $M$ features we need to estimate $K M$ parameters i.e. $\\beta_1,...,\\beta_M$ for each log-linear model for each class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use maximum likelihood to define the criteria we want to optimise on i.e. given a set of observations $(\\pmb{y}, \\pmb{X})$, how can we choose the params $\\pmb{\\beta}$ to maximise the likelihood of observing that data?\n",
    "\n",
    "We can write this as:\n",
    "\n",
    "$$L = \\prod_{i=1}^N p_1^{y_1} p_2^{y_2} \\cdots p_K^{y_K}$$\n",
    "\n",
    "where we have the following:\n",
    " - For each observation, compute the model probabilities for all K classes of belonging to that class ${p_1, p_2, \\cdots, p_K}$\n",
    " - Only include the probability $p_k$ if it is the true class i.e. if $y_k$ is the correct class (this is handled by exponent of $y_k$)\n",
    " - Do this for all observations from $i=1,\\cdots,N$\n",
    " - Multiply them altogether - this is the quantity we want to maximise\n",
    "\n",
    "Can see that this quantity has a maximum of N --> if we get it perfectly right every time with a model that spits out probability of 1 for the true class every time\n",
    "\n",
    "Can re-write as:\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\prod_{j=1}^K p_{ij}^{y_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quantity is then commonly taken logs of and negated so we have the 'cross-entropy' function which we then aim to minimise using a gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links:\n",
    "\n",
    "https://stackoverflow.com/questions/61662278/why-mnlogit-returns-classes-num-1-params-and-how-get-them-all\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
    "\n",
    "http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n",
    "\n",
    "http://www.utstat.toronto.edu/~rsalakhu/sta4273_2012/notes/Lecture3_2012.pdf\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "https://stats.stackexchange.com/questions/52104/multinomial-logistic-regression-vs-one-vs-rest-binary-logistic-regression\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Other Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression): a good article on what logistic regression is on stats exchange vs linear regression or pure class predictors like SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://stats.stackexchange.com/questions/949/when-is-logistic-regression-solved-in-closed-form): A good article on stats exchange on why logistic regression cannot be solved analytically like linear regression. This is due to the fact that the prediciton by construction is not linear in the parameters (as we squash it into the interval (0,1) and so cannot be solved using linear algebra). Instead we use a loss function defned by MLE and use gradient descent to find the optimal parameter set $(\\alpha, \\beta)$ for our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
